{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "888d44e1",
   "metadata": {
    "id": "40f3c663"
   },
   "source": [
    "# **Assignment 6 - Sentiment Analysis of Amazon Product Reviews**\n",
    "\n",
    "**_Neural Network and Deep Learning_**\n",
    "\n",
    "Group 3\n",
    " \n",
    "**Part 1** - Qichun Yu\n",
    "\n",
    "**Part 2** - Harjinder Singh Dhesi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2c9f5c",
   "metadata": {
    "id": "616eacb0"
   },
   "source": [
    "### Part 1\n",
    "\n",
    "**Question 1** - Create a Jupyter notebook and import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddeadbfc",
   "metadata": {
    "id": "bf5987c8"
   },
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655f0ce4",
   "metadata": {
    "id": "ba624312"
   },
   "source": [
    "**Question 2** - Read in the data files for the train and test sets (Amazon_reviews_train.csv and Amazon_reviews_test.csv). Examine the shapes of the datasets and print out the top five records from the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19010ca3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5536c0b",
    "outputId": "d6fd4e41-eb58-4585-a6f1-da631870d85b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training data: (25000, 2)\n",
      "The shape of the test data: (25000, 2)\n"
     ]
    }
   ],
   "source": [
    "# reading the train and test data files as pandas dataframes\n",
    "\n",
    "d_train = pd.read_csv('Amazon_reviews_train.csv')       # train data\n",
    "\n",
    "print('The shape of the training data:', d_train.shape) # shape of the train data\n",
    "\n",
    "d_test = pd.read_csv('Amazon_reviews_test.csv')         # test data\n",
    "\n",
    "print('The shape of the test data:', d_test.shape)      # shape of the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1ba58a",
   "metadata": {
    "id": "2cc71706"
   },
   "source": [
    "**Both train and test data have 25000 rows and 2 columns.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "028a8558",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "05a13441",
    "outputId": "801f062b-a533-4f19-e6d3-9b343de42852"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text  label\n",
       "0  Stuning even for the non-gamer: This sound tra...      1\n",
       "1  The best soundtrack ever to anything.: I'm rea...      1\n",
       "2  Amazing!: This soundtrack is my favorite music...      1\n",
       "3  Excellent Soundtrack: I truly like this soundt...      1\n",
       "4  Remember, Pull Your Jaw Off The Floor After He...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# displaying the top five records from the train data\n",
    "\n",
    "d_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec0aa244",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "6904b683",
    "outputId": "4305dfad-80af-4609-8545-7dbcc53cfcb8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Great CD: My lovely Pat has one of the GREAT v...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>One of the best game music soundtracks - for a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Batteries died within a year ...: I bought thi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>works fine, but Maha Energy is better: Check o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Great for the non-audiophile: Reviewed quite a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text  label\n",
       "0  Great CD: My lovely Pat has one of the GREAT v...      1\n",
       "1  One of the best game music soundtracks - for a...      1\n",
       "2  Batteries died within a year ...: I bought thi...      0\n",
       "3  works fine, but Maha Energy is better: Check o...      1\n",
       "4  Great for the non-audiophile: Reviewed quite a...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# displaying the top five records from the test data\n",
    "\n",
    "d_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f4bcae",
   "metadata": {
    "id": "bb761163"
   },
   "source": [
    "As can be seen that, our data has only **two columns, i.e., review text (our feature) and label (our target labels corresponding to each review)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def3a0a8",
   "metadata": {
    "id": "5d686248"
   },
   "source": [
    "**Question 3** - For convenience when it comes to processing, separate the raw text and the labels for the train and test set. Print the first two reviews from the train text. You should have the following four variables: train_raw comprising the raw text for the train data, train_labels with labels for the train data, test_raw containing raw text for the test data, and test_labels with labels for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8977254c",
   "metadata": {
    "id": "0ed88ac6"
   },
   "outputs": [],
   "source": [
    "# separating the raw text (reviews) and the labels in both train and test data as values\n",
    "\n",
    "train_raw = d_train.review_text.values      # train data reviews\n",
    "\n",
    "train_labels = d_train.label.values         # train data labels\n",
    "\n",
    "test_raw = d_test.review_text.values        # test data reviews\n",
    "\n",
    "test_labels = d_test.label.values           # test data labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66968e15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7a29c0de",
    "outputId": "c23a4495-acd8-4d7e-ae98-15063a97b503"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^'\n",
      " \"The best soundtrack ever to anything.: I'm reading a lot of reviews saying that this is the best 'game soundtrack' and I figured that I'd write a review to disagree a bit. This in my opinino is Yasunori Mitsuda's ultimate masterpiece. The music is timeless and I'm been listening to it for years now and its beauty simply refuses to fade.The price tag on this is pretty staggering I must say, but if you are going to buy any cd for this much money, this is the only one that I feel would be worth every penny.\"]\n"
     ]
    }
   ],
   "source": [
    "# printing the first two reviews from the train raw test\n",
    "\n",
    "print(train_raw[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d906b6",
   "metadata": {
    "id": "93a7171f"
   },
   "source": [
    "**Question 4** - Normalize the case and tokenize the test and train texts using NLTK's word_tokenize (after importing it, of course – hint: use list comprehension for cleaner code). Print the first review from the train data to check if the tokenization worked. Download punkt from NLTK if it is not already installed in your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dae5728b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9be7851",
    "outputId": "c789e32e-7ba0-445e-86b0-904be99f0ec1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/qichun/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importing word tokenizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c962c840",
   "metadata": {
    "id": "baa3c8bf"
   },
   "outputs": [],
   "source": [
    "# tokenizng the raw text (reviews) from both train and test data\n",
    "\n",
    "train_tokens = [word_tokenize(review.lower()) for review in train_raw]   # tokens of train texts\n",
    "\n",
    "test_tokens = [word_tokenize(review.lower()) for review in test_raw]     # tokens of test texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3eca5c43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7cecde95",
    "outputId": "7a8a80b7-c0e9-4c85-ea51-0ea5f56017bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stuning', 'even', 'for', 'the', 'non-gamer', ':', 'this', 'sound', 'track', 'was', 'beautiful', '!', 'it', 'paints', 'the', 'senery', 'in', 'your', 'mind', 'so', 'well', 'i', 'would', 'recomend', 'it', 'even', 'to', 'people', 'who', 'hate', 'vid', '.', 'game', 'music', '!', 'i', 'have', 'played', 'the', 'game', 'chrono', 'cross', 'but', 'out', 'of', 'all', 'of', 'the', 'games', 'i', 'have', 'ever', 'played', 'it', 'has', 'the', 'best', 'music', '!', 'it', 'backs', 'away', 'from', 'crude', 'keyboarding', 'and', 'takes', 'a', 'fresher', 'step', 'with', 'grate', 'guitars', 'and', 'soulful', 'orchestras', '.', 'it', 'would', 'impress', 'anyone', 'who', 'cares', 'to', 'listen', '!', '^_^']\n"
     ]
    }
   ],
   "source": [
    "# printing the first tokenized review of the train data\n",
    "\n",
    "print(train_tokens[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c728a411",
   "metadata": {
    "id": "5c88b6b9"
   },
   "source": [
    "It can be seen that the **raw text has been tokenized as it was supposed**.\n",
    "\n",
    "**Question 5** - Import stopwords (built in to NLTK) and punctuation from the string module. Define a function (drop_stop) to remove these tokens from any input tokenized sentence. Download stopwords from NLTK if it is not already installed in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "001f6846",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "be8d2e5f",
    "outputId": "7db0f2ab-1c87-48aa-ae9e-b75aef3f4cb8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/qichun/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importing stop words and punctuation for dropping these words from the raw text\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2304ca74",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b37b0ce1",
    "outputId": "9f20a4ce-20e3-469c-d422-72d735443875"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "# a list of english stop words\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "# a list of all possible punctuation marks\n",
    "\n",
    "stop_punct = list(punctuation)\n",
    "\n",
    "# a combined list of all words (stop words and punctuation) that need to be removed\n",
    "\n",
    "stop_final = stop_words + stop_punct\n",
    "\n",
    "print(stop_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cbbc2a3",
   "metadata": {
    "id": "65fe88c2"
   },
   "outputs": [],
   "source": [
    "# a function that drops the above stop words and punctuation marks from the raw text\n",
    "\n",
    "def drop_stop(input_tokens):\n",
    "    return [token for token in input_tokens if token not in stop_final]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a97bbc",
   "metadata": {
    "id": "82506e2f"
   },
   "source": [
    "**Question 6** - Using the defined function (drop_stop), remove the redundant stop words from the train and the test texts. Print the first review of the processed train texts to check if the function worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9836d89b",
   "metadata": {
    "id": "cd4ff535"
   },
   "outputs": [],
   "source": [
    "# removing the redundant stop words from train and test texts\n",
    "\n",
    "train_tokens_nostop = [drop_stop(tokens) for tokens in train_tokens]\n",
    "\n",
    "test_tokens_nostop = [drop_stop(tokens) for tokens in test_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83c0545e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "666422a6",
    "outputId": "094bc666-473d-47dc-9bab-73e0b278356e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stuning', 'even', 'non-gamer', 'sound', 'track', 'beautiful', 'paints', 'senery', 'mind', 'well', 'would', 'recomend', 'even', 'people', 'hate', 'vid', 'game', 'music', 'played', 'game', 'chrono', 'cross', 'games', 'ever', 'played', 'best', 'music', 'backs', 'away', 'crude', 'keyboarding', 'takes', 'fresher', 'step', 'grate', 'guitars', 'soulful', 'orchestras', 'would', 'impress', 'anyone', 'cares', 'listen', '^_^']\n"
     ]
    }
   ],
   "source": [
    "# printing the first tokenized review of the train data without all the stop words\n",
    "\n",
    "print(train_tokens_nostop[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5902091f",
   "metadata": {
    "id": "0e6d59d9"
   },
   "source": [
    "It can be observed that, **all the stop words and punctuations have been removed from the text**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7d1226",
   "metadata": {
    "id": "9877b99a"
   },
   "source": [
    "**Question 7** - Using Porter Stemmer from NLTK, stem the tokens for the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "507539c9",
   "metadata": {
    "id": "31cc59ac"
   },
   "outputs": [],
   "source": [
    "# importing Porter Stemmer from NLTK to return the stems of the words in train and test text\n",
    "\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46665486",
   "metadata": {
    "id": "edfa795b"
   },
   "outputs": [],
   "source": [
    "# creating a Porter Stemmer \n",
    "\n",
    "stemmer_p = PorterStemmer()\n",
    "\n",
    "# stemming of the train text\n",
    "\n",
    "train_tokens_stem = [[stemmer_p.stem(token) for token in tokens] for tokens in train_tokens_nostop]\n",
    "\n",
    "# stemming of the test text\n",
    "\n",
    "test_tokens_stem = [[stemmer_p.stem(token) for token in tokens] for tokens in test_tokens_nostop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a972ca2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "53c62fa5",
    "outputId": "8609e86f-f127-4c46-804f-b79130acead0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stune', 'even', 'non-gam', 'sound', 'track', 'beauti', 'paint', 'seneri', 'mind', 'well', 'would', 'recomend', 'even', 'peopl', 'hate', 'vid', 'game', 'music', 'play', 'game', 'chrono', 'cross', 'game', 'ever', 'play', 'best', 'music', 'back', 'away', 'crude', 'keyboard', 'take', 'fresher', 'step', 'grate', 'guitar', 'soul', 'orchestra', 'would', 'impress', 'anyon', 'care', 'listen', '^_^']\n"
     ]
    }
   ],
   "source": [
    "# printing the tokens of first review of the train data reduced their stems\n",
    "\n",
    "print(train_tokens_stem[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bb2e0c",
   "metadata": {
    "id": "2016d8be"
   },
   "source": [
    "Though some of the words doesn't make sense after the above step, but **the Stemming step reduced the words in the text to their stems**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e017e26",
   "metadata": {
    "id": "24d1b8b6"
   },
   "source": [
    "**Question 8** - Create the strings for each of the train and text reviews. This will help us work with the utilities in Keras to create and pad the sequences. Create the train_texts and test_texts variables. Print the first review from the processed train data to confirm it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72439a9c",
   "metadata": {
    "id": "ee6c5b62"
   },
   "outputs": [],
   "source": [
    "# joining the tokens and creating strings of each train and test reviews\n",
    "\n",
    "train_texts = [\" \".join(txt) for txt in train_tokens_stem]\n",
    "\n",
    "test_texts = [\" \".join(txt) for txt in test_tokens_stem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1860165a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "938038b6",
    "outputId": "4de1fd75-b00b-4d6c-a966-9dc01950f62c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stune even non-gam sound track beauti paint seneri mind well would recomend even peopl hate vid game music play game chrono cross game ever play best music back away crude keyboard take fresher step grate guitar soul orchestra would impress anyon care listen ^_^\n"
     ]
    }
   ],
   "source": [
    "# printing the first review from the processed train data\n",
    "\n",
    "print(train_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846687a6",
   "metadata": {
    "id": "f640f8fd"
   },
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a169a9f7",
   "metadata": {
    "id": "7e1e9a67"
   },
   "source": [
    "**Question 1** - From the Keras preprocessing utilities for text (keras.preprocessing.text), import the Tokenizer module. Define a vocabulary size of 10000 and instantiate the tokenizer with this vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96efc9d7",
   "metadata": {
    "id": "5fa8e2f3"
   },
   "outputs": [],
   "source": [
    "# importing Tokenizer module form keras.preprocessing.text\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86addbd6",
   "metadata": {
    "id": "1bcc7747"
   },
   "outputs": [],
   "source": [
    "# setting up a vocabulary size\n",
    "\n",
    "vocab_size = 10000\n",
    "\n",
    "# instantiating the tokenizer with the above vocab size\n",
    "\n",
    "tok = Tokenizer(num_words = vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45a4b09",
   "metadata": {
    "id": "0b69a941"
   },
   "source": [
    "**Question 2** - Fit the tokenizer on the train texts. This works just like CountVectorizer did in Chapter 4, Deep Learning for Text – Embeddings, and trains the vocabulary. After fitting, use the texts_to_sequences method of the tokenizer on the train and test sets to create the sequences for them. Print the sequence for the first review in the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a12d249c",
   "metadata": {
    "id": "6afc05fe"
   },
   "outputs": [],
   "source": [
    "# fitting the tokenizer created above on train texts\n",
    "\n",
    "tok.fit_on_texts(train_texts)\n",
    "\n",
    "# creating sequences for train and test data sets using text_to_sequence method\n",
    "\n",
    "train_sequences = tok.texts_to_sequences(train_texts)\n",
    "\n",
    "test_sequences = tok.texts_to_sequences(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d55e8632",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "793730c2",
    "outputId": "e14d2231-bb07-4b29-fa97-e0eba1d1e559"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 516, 7596, 85, 190, 184, 1097, 282, 20, 11, 1264, 22, 56, 370, 9662, 114, 41, 71, 114, 8159, 1453, 114, 51, 71, 29, 41, 58, 182, 2928, 2151, 76, 8160, 816, 2663, 829, 718, 3869, 11, 483, 120, 268, 109]\n"
     ]
    }
   ],
   "source": [
    "# printing the sequence for the first review in the train data\n",
    "\n",
    "print(train_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5cc24a",
   "metadata": {
    "id": "fbd17be5"
   },
   "source": [
    "The above step transforms each text in texts to a sequence of integers. It basically takes each word in the text and replaces it with its corresponding index in the vocabulary dictionary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c325309e",
   "metadata": {
    "id": "8200566b"
   },
   "source": [
    "**Question 3** - We need to find the optimal length of the sequences to process in the model. Get the length of the reviews from the train set into a list and plot the histogram of the lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5fdb9d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "df597620",
    "outputId": "5c6e814b-5628-41ae-e76e-89db10ae0353"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# creating a list of lengths of sequences for each revuew in the train set\n",
    "\n",
    "seq_lens = [len(seq) for seq in train_sequences]\n",
    "\n",
    "# plotting a histogram for the above lengths \n",
    "\n",
    "plt.hist(seq_lens)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc7f6e5",
   "metadata": {
    "id": "589debdf"
   },
   "source": [
    "**Question 4** - The data is now in the same format as the IMDb data we used in the chapter. Using a sequence length of 100 (define the maxlen = 100 variable), use the pad_sequences method from the sequence module in Keras' preprocessing utilities (keras.preprocessing.sequence) to limit the sequences to 100 for both the train and test data. Check the shape of the result for the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "679ed50a",
   "metadata": {
    "id": "29c6a1b1"
   },
   "outputs": [],
   "source": [
    "# setting the standard sequence length equal to 100\n",
    "\n",
    "max_seq_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5733a343",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "75cec0d5",
    "outputId": "25443271-29c4-414f-f13d-a54f9ffe529c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of resulting train data: (25000, 100)\n",
      "The shape of resulting test data: (25000, 100)\n"
     ]
    }
   ],
   "source": [
    "# importing pad_sequences method to limit all the sequences to a specific length of 100\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# padding train data sequences\n",
    "\n",
    "X_train = pad_sequences(train_sequences, maxlen = max_seq_len)\n",
    "\n",
    "print('The shape of resulting train data:', X_train.shape)\n",
    "\n",
    "# padding test data sequences \n",
    "\n",
    "X_test = pad_sequences(test_sequences, maxlen = max_seq_len)\n",
    "\n",
    "print('The shape of resulting test data:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4378859e",
   "metadata": {
    "id": "2ad55ec2"
   },
   "source": [
    "The above method padded the sequences, the lengths of which were shorter than the max specified length and truncated those with length longer than the max specified length to make all the sequences of same length.\n",
    "\n",
    "**It can be seen that the length of sequences of all the train data reviews has been made consistent, i.e., = 100**.\n",
    "\n",
    "\n",
    "**Question 5** - To build the model, import all the necessary layers from Keras (embedding, spatialdropout, LSTM, dropout, and dense) and import the Sequential model. Initialize the Sequential model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de8cd2ea",
   "metadata": {
    "id": "6170c5a4"
   },
   "outputs": [],
   "source": [
    "# importing all the necessary layers from keras to develop our model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, SpatialDropout1D, LSTM, GRU, Dropout, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2709b5ba",
   "metadata": {
    "id": "739bcaf8"
   },
   "outputs": [],
   "source": [
    "# initializing the Sequential Model\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72132bf6",
   "metadata": {
    "id": "daba463f"
   },
   "source": [
    "**Question 6** - Add an embedding layer with 32 as the vector size (output_dim). Add a spatial dropout of 40%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d26745f1",
   "metadata": {
    "id": "3f3a05b5"
   },
   "outputs": [],
   "source": [
    "# adding an embedding layer with output vector size = 32\n",
    "\n",
    "model.add(Embedding(vocab_size, output_dim = 32))\n",
    "\n",
    "# adding a spatial droupout of 40%\n",
    "\n",
    "model.add(SpatialDropout1D(0.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5c6c5d",
   "metadata": {
    "id": "b451a925"
   },
   "source": [
    "**Question 7** - Build a stacked LSTM model with 2 layers with 64 cells each. Add a dropout layer with 40% dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25ced205",
   "metadata": {
    "id": "d6af18fb"
   },
   "outputs": [],
   "source": [
    "# adding stacked LSTM layers to the model\n",
    "\n",
    "model.add(LSTM(64, return_sequences = True))\n",
    "model.add(LSTM(64, return_sequences = False))\n",
    "\n",
    "# adding dropout of 40%\n",
    "\n",
    "model.add(Dropout(0.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d98ba8",
   "metadata": {
    "id": "b5fdd88a"
   },
   "source": [
    "**Question 8** - Add a dense layer with 32 neurons with relu activation, then a 50% dropout layer, followed by another dense layer of 32 neurons with relu activation, and follow this up with another dropout layer with 50% dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2ba323e",
   "metadata": {
    "id": "f8fb8440"
   },
   "outputs": [],
   "source": [
    "# adding a dense layer with 32 neurons and relu activation function\n",
    "\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "\n",
    "# adding a dropout of 50%\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# adding another dense layer with 32 neurons and relu activation function\n",
    "\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "\n",
    "# adding a dropout of 50%\n",
    "\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4909eca7",
   "metadata": {
    "id": "195ac464"
   },
   "source": [
    "**Question 9** - Add a final dense layer with a single neuron with sigmoid activation and compile the model. Print the model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d434937d",
   "metadata": {
    "id": "999b4b83"
   },
   "outputs": [],
   "source": [
    "# adding the final dense layer with single layer and sigmoid activation function\n",
    "\n",
    "model.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f02450b7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bb215dea",
    "outputId": "95292bdf-197d-41f9-94bc-0db0f92651a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 32)          320000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 64)          24832     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 381,025\n",
      "Trainable params: 381,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# compiling the model \n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', \\\n",
    "              optimizer = 'rmsprop', \\\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "# printing the summary of the model\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e732e79e",
   "metadata": {
    "id": "c8d761d0"
   },
   "source": [
    "**Question 10** - Fit the model on the training data with a 20% validation split and a batch size of 128. Train for 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb97a03b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2b1ff629",
    "outputId": "5646d49c-935e-4278-9cc1-8ea1f732e814"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 32s 2ms/sample - loss: 0.5711 - accuracy: 0.6976 - val_loss: 0.4070 - val_accuracy: 0.8218\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 32s 2ms/sample - loss: 0.3660 - accuracy: 0.8601 - val_loss: 0.3824 - val_accuracy: 0.8318\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 30s 1ms/sample - loss: 0.3038 - accuracy: 0.8868 - val_loss: 0.3446 - val_accuracy: 0.8554\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 30s 1ms/sample - loss: 0.2680 - accuracy: 0.9015 - val_loss: 0.3908 - val_accuracy: 0.8544\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 32s 2ms/sample - loss: 0.2477 - accuracy: 0.9115 - val_loss: 0.3613 - val_accuracy: 0.8564\n"
     ]
    }
   ],
   "source": [
    "# fitting the model with 20% validation split, batch size of 128, and 5 epochs\n",
    "\n",
    "history_lstm = model.fit(X_train, train_labels, \\\n",
    "               batch_size = 128, \\\n",
    "               validation_split = 0.2, \\\n",
    "               epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208fb2ff",
   "metadata": {
    "id": "09976dde"
   },
   "source": [
    "**Question 11** - Make a prediction on the test set using the predict_classes method of the model. Using the accuracy_score method from scikit-learn, calculate the accuracy on the test set. Also, print out the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f18b210a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "id": "d6c317f8",
    "outputId": "878a5d32-9912-4628-8da4-d786e9915c04"
   },
   "outputs": [],
   "source": [
    "# creating the  prediction set using predict_classes method of the model\n",
    "\n",
    "test_pred = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ef1f3de",
   "metadata": {
    "id": "77596349"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10405  1752]\n",
      " [ 1614 11229]]\n"
     ]
    }
   ],
   "source": [
    "# importing confusion_matrix\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# printing the confusion matrix with true test labels and prediction set created above\n",
    "\n",
    "print(confusion_matrix(test_labels, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1489565c",
   "metadata": {
    "id": "92e1ffd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.86536\n"
     ]
    }
   ],
   "source": [
    "# printing the accuracy of the model on the test set\n",
    "\n",
    "print(accuracy_score(test_labels, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4228a9f",
   "metadata": {
    "id": "f40fd00b"
   },
   "source": [
    "**The accuracy of our stacked LSTM model came out to be 86.5% with the given hyperparameter values**. We can get even better results by further fine tuning those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57e3ce4",
   "metadata": {
    "id": "6bfa72fc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
